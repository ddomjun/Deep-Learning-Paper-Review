# Deep-Learning-Paper-Review
논문 리뷰

Attention Is All You Need

[Original Paper Link](https://arxiv.org/abs/1706.03762) | [Notion](https://careful-shape-b2c.notion.site/Attention-Is-All-You-Need-6ca67bc3e3a14c0db05daf138924368d?pvs=4)

Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

[Original Paper Link](https://arxiv.org/abs/1506.01497) | [Notion](https://careful-shape-b2c.notion.site/Faster-R-CNN-15895a4389a44aae87df8019fae2460f?pvs=4)

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[Original Paper Link](https://arxiv.org/abs/1810.04805) | [Notion](https://careful-shape-b2c.notion.site/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-cbbf706cde044cc1addb1cfe3c8a99b6?pvs=4)

A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)

[Original Paper Link](https://arxiv.org/abs/2002.05709) | [Notion](https://careful-shape-b2c.notion.site/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations-f4890775530246aa833f34debaa96f83?pvs=4)

BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

[Original Paper Link](https://arxiv.org/abs/1910.13461) | [Notion](https://careful-shape-b2c.notion.site/BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and--a88f1e69419040bfb0e512d67590205e?pvs=4)

Learning Transferable Visual Models From Natural Language Supervision (CLIP)

[Original Paper Link](https://arxiv.org/abs/2103.00020) | [Notion](https://careful-shape-b2c.notion.site/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision-0d4262a505814a51b2ce39abfa0ba238?pvs=4)

CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation

[Original Paper Link](https://arxiv.org/abs/2303.11797) | [Notion](https://careful-shape-b2c.notion.site/CAT-Seg-Cost-Aggregation-for-Open-Vocabulary-Semantic-Segmentation-350ff7fbee444baeae790f8ccc4a7603?pvs=4)
